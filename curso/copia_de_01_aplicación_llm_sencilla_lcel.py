# -*- coding: utf-8 -*-
"""Copia de 01_Aplicación_LLM_sencilla_LCEL

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TbRYXaacBK0WQwK5aRiAjbtH3h-U0p2V

# Aplicación LLM sencilla con LCEL

En esta guía te mostraremos cómo crear una aplicación LLM sencilla con LangChain. Esta aplicación traducirá texto del inglés a otro idioma. Se trata de una aplicación LLM relativamente sencilla: solo se necesita una única llamada LLM y algunas indicaciones. Aun así, es una excelente manera de comenzar a usar LangChain: se pueden crear muchas funciones con solo algunas indicaciones y una llamada LLM.
"""

!pip install langchain

"""## LangSmith
Muchas de las aplicaciones que cree con LangChain contendrán varios pasos con múltiples invocaciones de llamadas LLM. A medida que estas aplicaciones se vuelven cada vez más complejas, se vuelve crucial poder inspeccionar qué está sucediendo exactamente dentro de su cadena o agente. La mejor manera de hacerlo es con `LangSmith`.

"""

import getpass
import os

os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = getpass.getpass()

"""## Uso de modelos de lenguaje
En primer lugar, aprendamos a utilizar un modelo de lenguaje por sí solo. LangChain admite muchos modelos de lenguaje diferentes que puede utilizar de forma intercambiable. Seleccione el que desee utilizar a continuación.
"""

!pip install -qU langchain-openai

import getpass
import os

os.environ["OPENAI_API_KEY"] = getpass.getpass()

from langchain_openai import ChatOpenAI

model = ChatOpenAI(model="gpt-4")

"""Primero, usemos el modelo directamente. Los `ChatModels` son instancias de los `"Runnables"` de LangChain, lo que significa que exponen una interfaz estándar para interactuar con ellos. Para llamar al modelo de manera sencilla, podemos pasar una lista de mensajes al método `.invoke`."""

from langchain_core.messages import HumanMessage, SystemMessage

messages = [
    SystemMessage(content="Translate the following from Spanish into Korean"),
    HumanMessage(content="hola Grupo!,una traduccion simple y sencilla el cual esta conectado con langchain"),
]

model.invoke(messages)

"""# Analizadores de salida

Tenga en cuenta que la respuesta del modelo es un AIMessage. Este contiene una respuesta de cadena junto con otros metadatos sobre la respuesta. A menudo, es posible que solo queramos trabajar con la respuesta de cadena. Podemos analizar solo esta respuesta utilizando un analizador de salida simple.

Primero importamos el analizador de salida simple.
"""

from langchain_core.output_parsers import StrOutputParser

parser = StrOutputParser()

"""Una forma de utilizarlo es utilizarlo por sí solo. Por ejemplo, podríamos guardar el resultado de la llamada al modelo de lenguaje y luego pasarlo al analizador.


"""

result = model.invoke(messages)

parser.invoke(result)

"""Más comúnmente, podemos `"encadenar"` el modelo con este analizador de salida. Esto significa que este analizador de salida se llamará cada vez en esta cadena. Esta cadena toma el tipo de entrada del modelo de lenguaje (cadena o lista de mensajes) y devuelve el tipo de salida del analizador de salida (cadena).

Podemos crear fácilmente la cadena usando el operador `|`. El operador `|` se usa en LangChain para combinar dos elementos.
"""

chain = model | parser

chain.invoke(messages)

"""Si ahora observamos LangSmith, podemos ver que la cadena tiene dos pasos: primero se llama al modelo de lenguaje, luego se pasa el resultado de eso al analizador de salida. Podemos ver el rastro de LangSmith

# Plantillas de mensajes

En este momento, estamos pasando una lista de mensajes directamente al modelo de lenguaje. ¿De dónde proviene esta lista de mensajes? Por lo general, se construye a partir de una combinación de la entrada del usuario y la lógica de la aplicación. Esta lógica de la aplicación generalmente toma la entrada del usuario sin procesar y la transforma en una lista de mensajes listos para pasar al modelo de lenguaje. Las transformaciones comunes incluyen agregar un mensaje del sistema o formatear una plantilla con la entrada del usuario.

Las `PromptTemplates` son un concepto en LangChain diseñado para ayudar con esta transformación. Reciben la entrada del usuario sin procesar y devuelven datos (un mensaje) que están listos para pasar a un modelo de lenguaje.

Creemos una `PromptTemplate` aquí. Aceptará dos variables de usuario:

* `idioma:` el idioma al que se traducirá el texto
* `texto:` el texto a traducir
"""

from langchain_core.prompts import ChatPromptTemplate

"""Primero, crearemos una cadena que formatearemos para que sea el mensaje del sistema:"""

system_template = "Translate the following into {language}:"

"""A continuación, podemos crear la plantilla `PromptTemplate`. Esta será una combinación de la plantilla `system_template` y una plantilla más simple para ubicar el texto que se traducirá."""

prompt_template = ChatPromptTemplate.from_messages(
    [("system", system_template), ("user", "{text}")]
)

"""La entrada de esta plantilla de solicitud es un diccionario. Podemos jugar con esta plantilla de solicitud por sí sola para ver qué hace por sí sola."""

result = prompt_template.invoke({"language": "korean", "text": "hola Grupo!, estamos por terminar el curso y espero haya sido de utilidad y aprendizaje para todos!"})

result

"""Podemos ver que devuelve un `ChatPromptValue` que consta de dos mensajes. Si queremos acceder a los mensajes directamente hacemos:"""

result.to_messages()

"""# Encadenamiento de componentes con LCEL
Ahora podemos combinar esto con el modelo y el analizador de salida de arriba usando el operador de barra vertical `(|)`:
"""

chain = prompt_template | model | parser

chain.invoke({"language": "korean", "text": "hola Grupo!, estamos por termianr el curso y espero haya sido de utilidad y aprendizaje para todos!"})

"""Este es un ejemplo simple de cómo usar el lenguaje de expresión LangChain `(LCEL)` para encadenar módulos LangChain. Este enfoque tiene varias ventajas, como la compatibilidad optimizada con el seguimiento y la transmisión.

"""