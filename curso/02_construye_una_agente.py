# -*- coding: utf-8 -*-
"""02_Construye_una_agente

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bwMApNIon7p01djgeyccEUYL6P7D5jV2

# Construcción de una agente

Por sí solos, los modelos de lenguaje no pueden realizar acciones, solo generan texto. Un gran caso de uso para LangChain es la creación de agentes. Los agentes son sistemas que utilizan los modelos de lenguaje como motores de razonamiento para determinar qué acciones realizar y las entradas para pasarlas. Después de ejecutar las acciones, los resultados se pueden volver a introducir en el modelo de lenguaje para determinar si se necesitan más acciones o si está bien terminar.

En este tutorial, crearemos un agente que pueda interactuar con un motor de búsqueda. Podrás hacerle preguntas a este agente, verlo llamar a la herramienta de búsqueda y tener conversaciones con él.

## Instalación
Para instalar LangChain, ejecute:
"""

!pip install -U langchain-community langgraph tavily-python

"""## LangSmith

Muchas de las aplicaciones creadas con LangChain contendrán varios pasos con múltiples invocaciones de llamadas LLM. A medida que estas aplicaciones se vuelven cada vez más complejas, se vuelve crucial poder inspeccionar qué está sucediendo exactamente dentro de su cadena o agente. La mejor manera de hacerlo es con LangSmith.

Después de registrarse en el enlace anterior, asegúrese de configurar sus variables de entorno para comenzar a registrar los seguimientos:
"""

import getpass
import os

os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = getpass.getpass()

"""## Tavily
Usaremos Tavily (un motor de búsqueda) como herramienta. Para usarlo, necesitarás obtener y configurar una clave API:
"""

import getpass
import os

os.environ["TAVILY_API_KEY"] = getpass.getpass()

"""## Definir herramientas
Primero debemos crear las herramientas que queremos utilizar. Nuestra herramienta principal será Tavily, un motor de búsqueda. Tenemos una herramienta integrada en LangChain para utilizar fácilmente el motor de búsqueda Tavily como herramienta.
"""

from langchain_community.tools.tavily_search import TavilySearchResults

search = TavilySearchResults(max_results=2)
search_results = search.invoke("como está el clima en ciudad de méxico")
print(search_results)
# If we want, we can create other tools.
# Once we have all the tools we want, we can put them in a list that we will reference later.
tools = [search]

"""## Uso de modelos de lenguaje
A continuación, aprendamos a usar un modelo de lenguaje para llamar a herramientas. LangChain admite muchos modelos de lenguaje diferentes que puede usar de forma intercambiable. Seleccione el que desee usar a continuación.
"""

!pip install -qU langchain-openai

import getpass
import os

os.environ["OPENAI_API_KEY"] = getpass.getpass()

from langchain_openai import ChatOpenAI

model = ChatOpenAI(model="gpt-4")

"""Puedes llamar al modelo de lenguaje pasando una lista de mensajes. De manera predeterminada, la respuesta es una cadena de contenido `content`."""

from langchain_core.messages import HumanMessage

response = model.invoke([HumanMessage(content="hola!")])
response.content

"""Ahora podemos ver cómo es habilitar este modelo para que realice llamadas a herramientas. Para habilitarlo, usamos .`bind_tools` para darle al modelo de lenguaje el conocimiento de estas herramientas."""

model_with_tools = model.bind_tools(tools)

"""Ahora podemos llamar al modelo. Primero, llamémoslo con un mensaje normal y veamos cómo responde. Podemos observar tanto el campo de contenido como el campo de llamada de herramienta."""

response = model_with_tools.invoke([HumanMessage(content="Hola!")])

print(f"ContentString: {response.content}")
print(f"ToolCalls: {response.tool_calls}")

"""Ahora, intentemos llamarlo con alguna entrada que esperaría que se llame a una herramienta."""

response = model_with_tools.invoke([HumanMessage(content="What's the weather in SF?")])

print(f"ContentString: {response.content}")
print(f"ToolCalls: {response.tool_calls}")

"""Podemos ver que ahora no hay contenido de texto, pero hay una llamada a la herramienta. Quiere que llamemos a la herramienta de búsqueda de Tavily.

Esto todavía no llama a esa herramienta, solo nos lo dice. Para poder llamarla, necesitaremos crear nuestro agente.

## Crear el agente

Ahora que hemos definido las herramientas y el LLM, podemos crear el agente. Usaremos LangGraph para construir el agente. Actualmente, estamos usando una interfaz de alto nivel para construir el agente, pero lo bueno de LangGraph es que esta interfaz de alto nivel está respaldada por una API de bajo nivel, altamente controlable, en caso de que desee modificar la lógica del agente.

Ahora, podemos inicializar el agente con el LLM y las herramientas.

Tenga en cuenta que estamos pasando el `model`, no `model_with_tools`. Esto se debe a que `create_react_agent` llamará a `.bind_tools` por nosotros en segundo plano.
"""

from langgraph.prebuilt import create_react_agent

agent_executor = create_react_agent(model, tools)

"""## Ejecutar el agente

¡Ahora podemos ejecutar el agente en algunas consultas! Tenga en cuenta que, por ahora, todas son consultas sin estado (no recordará interacciones anteriores). Tenga en cuenta que el agente devolverá el estado final al final de la interacción (que incluye todas las entradas; veremos más adelante cómo obtener solo las salidas).

Primero, veamos cómo responde cuando no es necesario llamar a una herramienta:
"""

response = agent_executor.invoke({"messages": [HumanMessage(content="hi!")]})

response["messages"]

"""Para ver exactamente lo que está sucediendo en segundo plano (y para asegurarnos de que no está llamando a una herramienta), podemos echar un vistazo al seguimiento de LangSmith.

Probémoslo ahora en un ejemplo en el que debería estar invocando la herramienta.
"""

response = agent_executor.invoke(
    {"messages": [HumanMessage(content="whats the weather in sf?")]}
)
response["messages"]

"""Podemos verificar el seguimiento de LangSmith para asegurarnos de que esté llamando a la herramienta de búsqueda de manera efectiva.

##Mensajes de transmisión

Hemos visto cómo se puede llamar al agente con `.invoke` para obtener una respuesta final. Si el agente está ejecutando varios pasos, esto puede llevar un tiempo. Para mostrar el progreso intermedio, podemos transmitir mensajes a medida que ocurren.
"""

for chunk in agent_executor.stream(
    {"messages": [HumanMessage(content="whats the weather in sf?")]}
):
    print(chunk)
    print("----")

"""## Tokens de transmisión

Además de transmitir mensajes, también resulta útil transmitir tokens. Podemos hacerlo con el método .astream_events.

**Información**
Este método `.astream_events` solo funciona con `Python 3.11` o superior.
"""

async for event in agent_executor.astream_events(
    {"messages": [HumanMessage(content="whats the weather in sf?")]}, version="v1"
):
    kind = event["event"]
    if kind == "on_chain_start":
        if (
            event["name"] == "Agent"
        ):  # Was assigned when creating the agent with `.with_config({"run_name": "Agent"})`
            print(
                f"Starting agent: {event['name']} with input: {event['data'].get('input')}"
            )
    elif kind == "on_chain_end":
        if (
            event["name"] == "Agent"
        ):  # Was assigned when creating the agent with `.with_config({"run_name": "Agent"})`
            print()
            print("--")
            print(
                f"Done agent: {event['name']} with output: {event['data'].get('output')['output']}"
            )
    if kind == "on_chat_model_stream":
        content = event["data"]["chunk"].content
        if content:
            # Empty content in the context of OpenAI means
            # that the model is asking for a tool to be invoked.
            # So we only print non-empty content
            print(content, end="|")
    elif kind == "on_tool_start":
        print("--")
        print(
            f"Starting tool: {event['name']} with inputs: {event['data'].get('input')}"
        )
    elif kind == "on_tool_end":
        print(f"Done tool: {event['name']}")
        print(f"Tool output was: {event['data'].get('output')}")
        print("--")

"""## Adición de memoria

Como se mencionó anteriormente, este agente no tiene estado. Esto significa que no recuerda interacciones anteriores. Para darle memoria, necesitamos pasar un puntero de control. Cuando pasamos un puntero de control, también tenemos que pasar un `thread_id` al invocar al agente (para que sepa desde qué hilo/conversación continuar).
"""

!pip install langgraph

!pip install langgraph-checkpoint-sqlite

from langgraph.checkpoint.memory import MemorySaver
from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver
from langgraph.checkpoint.sqlite import SqliteSaver

from langgraph.checkpoint.sqlite import SqliteSaver

memory = SqliteSaver.from_conn_string(":memory:")

agent_executor = create_react_agent(model, tools, checkpointer=memory)

config = {"configurable": {"thread_id": "abc123"}}

for chunk in agent_executor.stream(
    {"messages": [HumanMessage(content="hi im bob!")]}, config
):
    print(chunk)
    print("----")

for chunk in agent_executor.stream(
    {"messages": [HumanMessage(content="whats my name?")]}, config
):
    print(chunk)
    print("----")

"""## Ejemplo de seguimiento de LangSmith

Si quiero iniciar una nueva conversación, todo lo que tengo que hacer es cambiar el `thread_id` utilizado
"""

config = {"configurable": {"thread_id": "xyz123"}}
for chunk in agent_executor.stream(
    {"messages": [HumanMessage(content="whats my name?")]}, config
):
    print(chunk)
    print("----")

"""## Conclusión
¡Eso es todo! En esta guía de inicio rápido, explicamos cómo crear un agente simple. Luego, mostramos cómo transmitir una respuesta, no solo los pasos intermedios, sino también los tokens. También agregamos memoria para que puedas tener una conversación con ellos. Los agentes son un tema complejo y hay mucho que aprender.
"""